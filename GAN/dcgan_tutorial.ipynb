{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import itertools\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.utils as vutils\n",
    "import torchvision.transforms as transforms\n",
    "from IPython import display\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 100 # 노이즈 벡터의 크기\n",
    "nc = 3 # 채널의 수\n",
    "ngf = 128 # generator 필터 조정\n",
    "ndf = 64 # discriminator 필터 조정\n",
    "niter = 200 # 에폭 수\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "\n",
    "imageSize = 64 # 만들어지는 이미지의 크기\n",
    "batchSize = 128 # 미니배치의 크기\n",
    "outf = \"result\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dglee\\appdata\\local\\conda\\conda\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Scale(64),\n",
    "        transforms.ToTensor(),                     \n",
    "        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_dataset = dsets.CIFAR10(root='./data/', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size= batchSize, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:         # Conv weight init\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:  # BatchNorm weight init\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netG, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "\n",
    "            # 입력값은 Z이며 Transposed Convolution을 거칩니다.\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf * 8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf * 4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf*2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # (ngf * 2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            # ngf x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # (nc) x 64 x 64)\n",
    "            nn.Conv2d(nc, ndf, 4,2,1,bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # ndf x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # (ndf * 2) x 16 x 16\n",
    "            nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # (ndf * 4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(ndf*8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So=stride*(Si−1)+Sf−2∗pad # deconvolution 공식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_netD(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netG = _netG()\n",
    "netG.apply(weights_init)\n",
    "\n",
    "netD = _netD()\n",
    "netD.apply(weights_init)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "input = torch.FloatTensor(batchSize, 3, imageSize, imageSize)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "fixed_noise = Variable(fixed_noise).cuda()\n",
    "\n",
    "#label = torch.FloatTensor(batchSize)\n",
    "#labelv_fake = torch.FloatTensor(batchSize)\n",
    "#print(noise.shape)\n",
    "#real_label = 1\n",
    "#fake_label = 0\n",
    "\n",
    "  \n",
    "netG.cuda()\n",
    "netD.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum(모멘텀)\n",
    "\n",
    " 모멘텀(momentum)이란 단어는 관성, 탄력, 가속도라는 뜻입니다. 모멘텀 SGD는 경사 하강법에 관성을 더해 주는 것입니다. 경사 하강법과 마찬가지로 매번 기울기를 구하지만, 가중치를 수정하기전 이전 수정 방향(+,-)를 참고하여 같은 방향으로 일정한 비율만 수정되게 하는 방법입니다. 수정이 양(+) 방향, 음(-) 방향 순차적으로 일어나는 지그재그 현상이 줄어들고, 이전 이동 값을 고려해여 일정 비율만큼 다음 값을 결정하므로 관성의 효과를 낼 수 있습니다.\n",
    "<img src=\"./image/Momenturm2.png\" />\n",
    "우선 아래 수식에서 α는 Learning Rate, m은 momentum 계수입니다. \n",
    "<img src=\"./image/Momenturm.png\" />\n",
    "m의 정확한 용어는 아니지만 저희는 그냥 모멘텀(운동량) 또는 모멘텀 계수라고 부릅니다. 보통 0.9로 설정하며 교차 검증을 한다면 0.5에서 시작하여 0.9, 0.95, 0.99 순서로 증가시켜 검증합니다. 예시로 맨 처음 gradient()의 값이 0.5이고 두 번째 gradient 값이 -0.3이라 할 때 m이 0.9라면 V(1)은 -0.5, V(2)는 0.9 * -0.5 +0.3 = -0.45 + 0.3 = -0.15이다. 이처럼 gradient의 방향이 변경되어도 이전 방향과 크기에 영향받아 다른 방향으로 가중치가 변경될 수 있습니다.\n",
    "python_code\n",
    "v = m * v - learning_rate * gradient\n",
    "weight[i] += v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSprop(알엠에스프롭)\n",
    "\n",
    "RMSprop(알엠에스프롭)은 아다그라드의 G(t)의 값이 무한히 커지는 것을 방지하고자 제안된 방법으로, 논문과 같은 형태로 발표된 다른 방법들과 달리 제프리 힌튼 교수와 제자들이 코세라(Coursera) 수업에서 소개하였습니다. 링크를 클릭하시면 영상 페이지로 들어가실수 있으며 이론과 수식 도출에 대해 설명하고 있습니다. RMSprop은 지수 이동평균을 이용한 방법입니다. \n",
    "우선 지수 이동평균에 대해 알아보겠습니다. 지수 이동평균이란 쉽게 말해 최근 값을 더 잘 반영하기 위해 최근 값에 값과 이전 값에 각각 가중치를 주어 계산한는 방법입니다. \n",
    "<img src=\"./image/rmsprop1.png\" />\n",
    "위 식에서 지수 이동편균값은 x, 현재 값은 p, 가중치는 α(알파)이며, 아래 첨자 k는 step 또는 시간, 마지막으로 N은 값의 개수라고 보시면 됩니다. 만약 처음부터 현재까지 계산을 하게 된다고 한다면 N과 k의 값은 같으며 가중치 α(알파)는 N이 작을 수록 커집니다. \n",
    "<img src=\"./image/rmsprop2.png\" />\n",
    "계산식을 풀어써 보면 위와 같은데 식을 보면 알 수 있듯이 1주기가 지날 때마다 (1-α)라는 가중치가 이전 값에 곱해지는데, (1-α) 값이 1보다 작기 때문에 시간이 지날수록 영향력이 줄어드는 효과를 볼 수 있습니다. 참고로 필터이론에서는 이런 가중치를 forgetting factor 또는 decaying factor라고 합니다.\n",
    "RMSprop 수식은 다음과 같습니다.\n",
    "<img src=\"./image/rmsprop3.png\" />\n",
    "기존 Adagrad에서는 G(t)를 구성하는 두 항이 그냥 더 해지지만  RMSprop에서는 지수평균으로 더해집니다.\n",
    "python_code\n",
    "g = gamma * g + (1 - gamma) * gradient**2\n",
    "weight[i] += -learning_rate * gradient / (np.sqrt(g) + e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam(Adaptive Moment Estimation, 아담)\n",
    "  Adam은 Momentum과 RMSprop를 합친? 경사 하강법입니다. RMSprop의 특징인 gradient의 제곱을 지수평균한 값을 사용하며 Momentum의 특징으로 gradient를 제곱하지 않은 값을 사용하여 지수평균을 구하고 수식에 활용합니다. \n",
    "수식은 다음과 같습니다.\n",
    "<img src=\"./image/adam1.png\" />\n",
    "기존 RMSprop과 momentum과 다르게 M(t)와 V(t)가 바로 W(t+1) 수식에 들어가는 것이 아니라 와 가 들어갑니다. 이 부분을 논문에서는 바이어스가 수정된 값으로 변경하는 과정이라고 합니다. 이전에 저희가 알아야할 것은 초기 M(0)와 V(0)값이 0으로 초기화 되는데 시작값이 0이기 때문에 이동평균을 구하면 0으로 편향된 값추정이 발생할 수 있습니다. 특히 초기 감쇠 속도가 작은 경우 (즉, β가 1에 가까울 때)에 발생합니다. 이를 방지하기 위해 값을 나누어 바이어스 보정을 해줍니다. 은 M(t)와 V(t)의 기대값을 구하는 과정에서 찾을 수 있다고 하는데 수학적 지식이 부족하여 어떻게 발견되는지 설명드리기가 힘들 것 같습니다. 추가적으로 α=0.001, β1로는 0.9, β2로는 0.999, ϵ 으로는 10^-8 값이 가장 좋은 Default값이라고 논문에 명시되어 있습니다. \n",
    "https://arxiv.org/pdf/1412.6980.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a5d761224269>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# setup optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0moptimizerD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0moptimizerG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.999\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Step': 99, 'loss_G': tensor(2.5199, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.0898, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(4.3914, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3922, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.7049, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.2662, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(1.9598, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.9632, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.8008, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.6367, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(6.2227, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(3.4370, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.1231, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.6399, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.2447, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.0286, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.2288, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.6055, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.4518, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7534, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.3550, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.8333, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.5479, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.4519, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.0911, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.4757, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.6802, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7315, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.0221, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7616, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(2.8069, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.4089, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(1.6985, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.8322, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(1.5263, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.0153, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(1.3950, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.1627, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(1.3011, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.9791, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(0.4949, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(2.2342, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(5.1846, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.2339, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.2014, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.5659, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(3.4562, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.6566, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(2.2727, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.6407, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.3667, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.9232, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(1.7877, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.3789, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(2.7761, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.6346, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(0.8864, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.1799, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.4946, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3575, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(1.6094, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.1825, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(1.2827, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.0240, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.7817, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.2638, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.7777, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7703, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(1.5809, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.1634, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.4022, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.6096, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.2731, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(5.6984, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.8226, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.2966, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1306, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(5.6670, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.4328, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(5.0775, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.6563, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(1.8382, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.4770, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.0813, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.2801, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.5224, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1418, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(3.0562, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.5649, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.0601, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.5605, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(1.3462, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7664, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.3309, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.8957, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(2.7898, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(3.3718, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.7147, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1420, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.9761, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0252, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(8.4425, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(3.9613, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.2580, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1295, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(3.4589, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.4506, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(4.2161, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0760, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(4.8498, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1110, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(1.0625, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.8884, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.4944, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3459, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.4856, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.8337, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(1.4893, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.9840, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(0.9531, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.1615, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.7543, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7232, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(3.3495, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1921, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(2.7201, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3973, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.0063, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.0024, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.8097, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.5044, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(4.6583, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1125, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.8055, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.4436, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.8639, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.3385, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(1.1257, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.2641, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.2454, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(2.1128, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(5.9047, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0344, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.1156, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1781, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(1.7723, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7517, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.0150, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7633, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(2.3319, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.0811, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(4.0490, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.2498, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.0891, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.6113, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(2.6911, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3801, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(4.3983, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0759, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(6.3275, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.5125, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(4.4984, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0346, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.7044, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1915, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.9038, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3211, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.6447, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1256, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.4400, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3544, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(3.4091, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1305, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(5.8401, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.2684, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.7115, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.7490, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.1361, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0568, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(5.3073, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0182, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.3382, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7041, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(0.9938, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(2.7710, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.6081, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.2002, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(4.7426, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.1870, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(6.4336, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0370, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.1187, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.1864, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(0.4383, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.8865, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(3.0366, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.9719, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(5.1104, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0331, device='cuda:0', grad_fn=<AddBackward0>)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Step': 199, 'loss_G': tensor(4.3776, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0999, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(3.1213, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3873, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(0.9103, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(2.3357, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(4.1411, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0851, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.4931, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0394, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(2.9940, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3018, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.6119, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.5027, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.1066, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0538, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.3811, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.4378, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.4675, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.5058, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.8829, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3195, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(4.1419, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.0707, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.7900, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.6125, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(7.2504, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0086, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(2.4857, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.8784, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.0246, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.2392, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(5.0985, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0766, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(5.3923, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.8485, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.3371, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7402, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(2.6420, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.2720, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(3.3937, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.8504, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(4.0793, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0597, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.0542, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0612, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(4.8929, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0312, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.9474, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0897, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(6.0013, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0072, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(5.1751, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(1.5347, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.9466, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3102, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.7905, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0532, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(6.1446, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0221, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(2.0135, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7292, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.6954, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0664, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(1.4119, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.5397, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.0472, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.4872, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.4503, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0634, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(2.2386, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.5400, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(3.2511, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3327, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(5.2800, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0122, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(1.8517, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.6827, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(1.3153, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.5758, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(5.4032, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0146, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(1.8141, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.8062, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(5.7935, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0336, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.8648, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0249, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(2.3942, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.7710, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(4.2431, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0885, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(3.4087, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.3290, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(6.7191, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0829, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(6.5245, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0058, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(6.7123, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0030, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(8.7149, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(8.7781, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(9.7000, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(9.4687, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(8.9253, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0017, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(10.9730, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0005, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(5.5188, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0077, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(6.6624, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0023, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(4.6462, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0123, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(11.1808, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(5.5430, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0067, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(5.7798, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0052, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(9.9811, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(9.9545e-05, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(7.7578, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0006, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(7.9908, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0007, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(9.0455, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(7.0590, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0013, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(9.1271, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(9.3726, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0001, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(11.8288, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(4.7117e-05, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(11.2810, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(3.6182e-05, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(8.8203, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0004, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 199, 'loss_G': tensor(10.7050, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(9.6452e-05, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 299, 'loss_G': tensor(8.5543, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0003, device='cuda:0', grad_fn=<AddBackward0>)}\n",
      "{'Step': 99, 'loss_G': tensor(9.0324, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward>), 'loss_D': tensor(0.0002, device='cuda:0', grad_fn=<AddBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "epochNum = 100installed_ros_if = any(tf);\n",
    "for epoch in range(epochNum):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "\n",
    "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "        #label.resize_(batch_size).fill_(real_label)\n",
    "        #labelv_fake.resize_(batch_size).fill_(fake_label)\n",
    "\n",
    "        inputv = Variable(input).cuda()\n",
    "        real_labels = Variable(torch.ones(batch_size)).cuda()\n",
    "        fake_labels = Variable(torch.zeros(batch_size)).cuda()\n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise).cuda()\n",
    "        \n",
    "        #labelv = Variable(label).cuda()\n",
    "        #labelv_fake = Variable(label_fake).cuda()\n",
    "\n",
    "        output = netD(inputv)\n",
    "        errD_real = criterion(output, real_labels)\n",
    "        errD_real.backward()\n",
    "        #D_x = output.data.mean()\n",
    "\n",
    "        # train with fake\n",
    "       \n",
    "        fake = netG(noisev)\n",
    "        #labelv = Variable(label.fill_(fake_label)).cuda()\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, fake_labels)\n",
    "        errD_fake.backward()\n",
    "        #D_G_z1 = output.data.mean()\n",
    "\n",
    "        errD = errD_real + errD_fake\n",
    "        #errD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        netG.zero_grad()\n",
    "        #labelv = Variable(label.fill_(real_label)).cuda()\n",
    "        output = netD(fake)\n",
    "\n",
    "        errG = criterion(output, real_labels)\n",
    "        errG.backward()\n",
    "        #D_G_z2 = output.data.mean()\n",
    "        optimizerG.step()\n",
    "        if ((i+1) % 100 == 0):\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.data,'./dcgan_result/fake_samples_epoch_%s.png' % (str(epoch)+\" \"+str(i+1)),normalize=True)\n",
    "            vutils.save_image(real_cpu,'./dcgan_result/real_samples.png',normalize=True)\n",
    "            result_dict = {\"Step\":i,\"loss_D\":errD,\"loss_G\":errG}\n",
    "            print(result_dict)\n",
    "    #do checkpointing\n",
    "    torch.save(netG.state_dict(), './dcgan_result/netG.pth')\n",
    "    torch.save(netD.state_dict(), './dcgan_result/netD.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
